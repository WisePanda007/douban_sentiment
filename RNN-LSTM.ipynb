{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 待改进部分\n",
    " 1 过拟合 调参  \n",
    " 2 数据不均衡：（1）使用下采样（2）使用auc分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "day=time.strftime(\"%Y-%m-%d\", time.localtime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cdbd2cda1cf18259c85b40e</td>\n",
       "      <td>当然算不上杰作，但放豆瓣语境下，是部时至今日终于拍出来的国产“高分韩国电影”。拿现实题材拍商...</td>\n",
       "      <td>[算不上, 杰作, 但放, 豆瓣, 语境, 时至今日, 终于, 国产, 高分, 韩国, 电影...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cdbd2cda1cf18259c85b40f</td>\n",
       "      <td>最大的病，其实是穷病。真的被感动了，整体都很成熟，也有些许韩片的影子。几个演员表演都非常出色...</td>\n",
       "      <td>[最大, 其实, 穷病, 真的, 感动, 整体, 成熟, 些许, 韩片, 影子, 几个, 演...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5cdbd2cda1cf18259c85b40e   \n",
       "1  5cdbd2cda1cf18259c85b40f   \n",
       "\n",
       "                                             content  \\\n",
       "0  当然算不上杰作，但放豆瓣语境下，是部时至今日终于拍出来的国产“高分韩国电影”。拿现实题材拍商...   \n",
       "1  最大的病，其实是穷病。真的被感动了，整体都很成熟，也有些许韩片的影子。几个演员表演都非常出色...   \n",
       "\n",
       "                                   processed_comment  star  \n",
       "0  [算不上, 杰作, 但放, 豆瓣, 语境, 时至今日, 终于, 国产, 高分, 韩国, 电影...     4  \n",
       "1  [最大, 其实, 穷病, 真的, 感动, 整体, 成熟, 些许, 韩片, 影子, 几个, 演...     4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#从数据库中读取数据\n",
    "client=pymongo.MongoClient('localhost',27017)#连接数据库\n",
    "db1=client['douban']#创建新数据库\n",
    "pred_comments=db1['pred_comments']\n",
    "data = pd.DataFrame(list(pred_comments.find()))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>processed_comment</th>\n",
       "      <th>star</th>\n",
       "      <th>word_vec</th>\n",
       "      <th>pos1neg0</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cdbd2cda1cf18259c85b40e</td>\n",
       "      <td>当然算不上杰作，但放豆瓣语境下，是部时至今日终于拍出来的国产“高分韩国电影”。拿现实题材拍商...</td>\n",
       "      <td>[算不上, 杰作, 但放, 豆瓣, 语境, 时至今日, 终于, 国产, 高分, 韩国, 电影...</td>\n",
       "      <td>4</td>\n",
       "      <td>[[0.192524, -0.185849, 0.275798, 0.053039, 0.5...</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5cdbd2cda1cf18259c85b40f</td>\n",
       "      <td>最大的病，其实是穷病。真的被感动了，整体都很成熟，也有些许韩片的影子。几个演员表演都非常出色...</td>\n",
       "      <td>[最大, 其实, 穷病, 真的, 感动, 整体, 成熟, 些许, 韩片, 影子, 几个, 演...</td>\n",
       "      <td>4</td>\n",
       "      <td>[[-0.026906, 0.003273, 0.084483, 0.253033, 0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  5cdbd2cda1cf18259c85b40e   \n",
       "1  5cdbd2cda1cf18259c85b40f   \n",
       "\n",
       "                                             content  \\\n",
       "0  当然算不上杰作，但放豆瓣语境下，是部时至今日终于拍出来的国产“高分韩国电影”。拿现实题材拍商...   \n",
       "1  最大的病，其实是穷病。真的被感动了，整体都很成熟，也有些许韩片的影子。几个演员表演都非常出色...   \n",
       "\n",
       "                                   processed_comment  star  \\\n",
       "0  [算不上, 杰作, 但放, 豆瓣, 语境, 时至今日, 终于, 国产, 高分, 韩国, 电影...     4   \n",
       "1  [最大, 其实, 穷病, 真的, 感动, 整体, 成熟, 些许, 韩片, 影子, 几个, 演...     4   \n",
       "\n",
       "                                            word_vec  pos1neg0  seq_length  \n",
       "0  [[0.192524, -0.185849, 0.275798, 0.053039, 0.5...         1          27  \n",
       "1  [[-0.026906, 0.003273, 0.084483, 0.253033, 0.0...         1          30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors=np.load('sample_Tencent_AILab_ChineseEmbedding.npy',allow_pickle=True).item()\n",
    "#转换词向量\n",
    "def convert_to_vec(words_list):\n",
    "    words_vec=[]\n",
    "    for i in words_list:\n",
    "        if i in word_vectors:\n",
    "            words_vec.append(np.array(word_vectors[i]))\n",
    "    return np.array(words_vec)\n",
    "#获取每个评论词向量的数量\n",
    "def get_seq_length(data):\n",
    "    return len(data)\n",
    "# 转换评星\n",
    "def convert_stars(star):\n",
    "    if int(star)>3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['word_vec']=data.processed_comment.apply(convert_to_vec)\n",
    "data['pos1neg0']=data.star.apply(convert_stars)\n",
    "data['seq_length']=data.word_vec.apply(get_seq_length)\n",
    "data=data[[list(i)!=[] for i in data['word_vec']]]#删除词向量为[]的文本\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star</th>\n",
       "      <th>pos1neg0</th>\n",
       "      <th>seq_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>83247.000000</td>\n",
       "      <td>83247.000000</td>\n",
       "      <td>83247.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.028289</td>\n",
       "      <td>0.845075</td>\n",
       "      <td>16.523875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.126239</td>\n",
       "      <td>0.361835</td>\n",
       "      <td>14.875728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>930.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               star      pos1neg0    seq_length\n",
       "count  83247.000000  83247.000000  83247.000000\n",
       "mean       4.028289      0.845075     16.523875\n",
       "std        1.126239      0.361835     14.875728\n",
       "min        1.000000      0.000000      1.000000\n",
       "25%        4.000000      1.000000      5.000000\n",
       "50%        4.000000      1.000000     12.000000\n",
       "75%        5.000000      1.000000     25.000000\n",
       "max        5.000000      1.000000    930.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均每个评论有 17.11447859982942 个词\n",
      "平均每个评论有 16.523874734224655 个 wordvec\n"
     ]
    }
   ],
   "source": [
    "num2=np.mean(data[:].processed_comment.apply(lambda x:len(x)))\n",
    "num1=np.mean(data[:].word_vec.apply(lambda x:len(x)))\n",
    "print('平均每个评论有',num2,'个词')\n",
    "print('平均每个评论有',num1,'个 wordvec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#下采样\n",
    "negdata=data[~data['pos1neg0'].isin([1])]\n",
    "posdata=data[~data['pos1neg0'].isin([0])]\n",
    "xiacaiyangpos=posdata.sample(frac=0.25,replace=False)\n",
    "newdata=pd.concat( [negdata,xiacaiyangpos], axis=0 ) \n",
    "data=newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据总数量： 30485\n",
      "负类所占比例： 0.42306052156798424\n",
      "训练集数量： 24388\n",
      "测试集数量： 6097\n",
      "训练集负类所占比例： 0.4230769230769231\n",
      "测试集负类所占比例： 0.42299491553222895\n"
     ]
    }
   ],
   "source": [
    "X=np.array(data['word_vec'])\n",
    "num=30#要保留的维度，不足补0\n",
    "X_new=np.zeros((len(X),num,200))\n",
    "for i,j in enumerate(X):\n",
    "    if j.shape[0]>=num:\n",
    "        X_new[i]=X[i][:num]\n",
    "    else:\n",
    "        X_new[i]=np.concatenate([X[i],np.zeros(((num-j.shape[0]),200))])       \n",
    "X=X_new\n",
    "y=np.array(data['pos1neg0'])\n",
    "seq=np.array(data['seq_length'])\n",
    "# for i,j in enumerate(seq):\n",
    "#     if j>num:\n",
    "#         seq[i]=num\n",
    "\n",
    "print('数据总数量：',y.shape[0])\n",
    "print('负类所占比例：',list(y).count(0)/(list(y).count(1)+list(y).count(0)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test,seq_train,seq_test=train_test_split(X,np.array(y),seq,test_size=0.2)\n",
    "print('训练集数量：',y_train.shape[0])\n",
    "print('测试集数量：',y_test.shape[0])\n",
    "print('训练集负类所占比例：',list(y_train).count(0)/(list(y_train).count(1)+list(y_train).count(0)))\n",
    "print('测试集负类所占比例：',list(y_test).count(0)/(list(y_test).count(1)+list(y_test).count(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, seq, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch,seq_batch = X[batch_idx], y[batch_idx],seq[batch_idx]\n",
    "        yield X_batch, y_batch,seq_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-d5bdfe3cfdc3>:13: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-d5bdfe3cfdc3>:16: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\Program\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From D:\\Program\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-10-d5bdfe3cfdc3>:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "n_steps = num\n",
    "n_inputs=200\n",
    "n_neurons = 64\n",
    "n_outputs = 2\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, None,n_inputs],name='X')\n",
    "y = tf.placeholder(tf.int32, [None],name='y')\n",
    "# seq_length = tf.placeholder(tf.int32, [None], name=\"seq_length\")\n",
    "\n",
    "with tf.name_scope('RNN'):\n",
    "    lstmCell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)\n",
    "    # 配置dropout参数，以此避免过拟合\n",
    "    droplstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "    outputs, states = tf.nn.dynamic_rnn(droplstmCell, X, dtype=tf.float32)#,sequence_length=seq_length)\n",
    "    \n",
    "with tf.name_scope('Loss'):\n",
    "    logits = tf.layers.dense(outputs[:,-1,:],n_outputs)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    loss_summary = tf.summary.scalar('log_loss', loss)#使用tensorboard\n",
    "\n",
    "learning_rate = 0.001\n",
    "with tf.name_scope('Train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "with tf.name_scope(\"Eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "file_writer = tf.summary.FileWriter('douban_log/'+day, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tValidation accuracy: 73.000% \tLoss: 0.56698 \tTest accuracy: 77.612%\n",
      "Epoch: 1 \tValidation accuracy: 76.000% \tLoss: 0.46372 \tTest accuracy: 75.513%\n",
      "Epoch: 2 \tValidation accuracy: 79.000% \tLoss: 0.44231 \tTest accuracy: 79.482%\n",
      "Epoch: 3 \tValidation accuracy: 81.000% \tLoss: 0.43146 \tTest accuracy: 78.760%\n",
      "Epoch: 4 \tValidation accuracy: 76.000% \tLoss: 0.46080 \tTest accuracy: 79.449%\n",
      "Epoch: 5 \tValidation accuracy: 82.000% \tLoss: 0.36151 \tTest accuracy: 80.384%\n",
      "Epoch: 6 \tValidation accuracy: 80.000% \tLoss: 0.53343 \tTest accuracy: 81.450%\n",
      "Epoch: 7 \tValidation accuracy: 83.000% \tLoss: 0.44729 \tTest accuracy: 80.417%\n",
      "Epoch: 8 \tValidation accuracy: 88.000% \tLoss: 0.25400 \tTest accuracy: 80.761%\n",
      "Epoch: 9 \tValidation accuracy: 89.000% \tLoss: 0.32133 \tTest accuracy: 80.433%\n",
      "Epoch: 10 \tValidation accuracy: 92.000% \tLoss: 0.23793 \tTest accuracy: 80.892%\n",
      "Epoch: 11 \tValidation accuracy: 89.000% \tLoss: 0.22683 \tTest accuracy: 80.663%\n",
      "Epoch: 12 \tValidation accuracy: 84.000% \tLoss: 0.33554 \tTest accuracy: 79.957%\n",
      "Epoch: 13 \tValidation accuracy: 87.000% \tLoss: 0.33907 \tTest accuracy: 80.335%\n"
     ]
    }
   ],
   "source": [
    "#定义mini-batch大小与循环轮次\n",
    "n_epochs = 200\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "#保存检查点\n",
    "checkpoint_path = \"tmp_model/\"+day+\"/my_douban_rnn_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"model/\"+day+\"/my_douban_rnn_model\"\n",
    "\n",
    "#如果出现50次loss大于之前最好的损失，提前停止\n",
    "best_loss = np.infty\n",
    "epochs_without_progress = 0\n",
    "max_epochs_without_progress = 20\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if os.path.isfile(checkpoint_epoch_path):\n",
    "        # if the checkpoint file exists, restore the model and load the epoch number\n",
    "        with open(checkpoint_epoch_path, \"rb\") as f:\n",
    "            start_epoch = int(f.read())\n",
    "        print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        sess.run(init)\n",
    "        \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for X_batch, y_batch,seq_batch in shuffle_batch(X_train, y_train, seq, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})#, seq_length:seq_batch})\n",
    "        \n",
    "        accuracy_val, loss_val, accuracy_summary_str, loss_summary_str = sess.run(\n",
    "            [accuracy, loss, accuracy_summary, loss_summary], feed_dict={X: X_batch, y: y_batch}#, seq_length:seq_batch}\n",
    "        )\n",
    "        file_writer.add_summary(accuracy_summary_str, epoch)\n",
    "        file_writer.add_summary(loss_summary_str, epoch)\n",
    "        acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})#,seq_length:seq_test})\n",
    "        print(\"Epoch:\", epoch,\"\\tValidation accuracy: {:.3f}%\".format(accuracy_val * 100),\"\\tLoss: {:.5f}\".format(loss_val),\n",
    "             \"\\tTest accuracy: {:.3f}%\".format(acc_test * 100))\n",
    "        saver.save(sess, checkpoint_path)\n",
    "        with open(checkpoint_epoch_path, \"wb\") as f:\n",
    "            f.write(b\"%d\" % (epoch + 1))\n",
    "        if loss_val < best_loss:\n",
    "            saver.save(sess, final_model_path)\n",
    "            best_loss = loss_val\n",
    "        else:\n",
    "            epochs_without_progress += 1\n",
    "            if epochs_without_progress > max_epochs_without_progress:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    accuracy_val = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    logits1 = logits.eval(feed_dict={X: X_train, y: y_train})\n",
    "    logits2 = logits.eval(feed_dict={X: X_test, y: y_test})\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_train_pre=[0 if i[0]>i[1] else 1 for i in logits1]\n",
    "y_test_pre=[0 if i[0]>i[1] else 1 for i in logits2]\n",
    "print('训练集: \\n',confusion_matrix(y_train,y_train_pre))\n",
    "print('测试集: \\n',confusion_matrix(y_test,y_test_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_path = \"model/my_douban_rnn_model\"\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, final_model_path)\n",
    "    output = outputs.eval(feed_dict={X: X_train, y: y_train})\n",
    "#     states = states.eval(feed_dict={X: X_train, y: y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
